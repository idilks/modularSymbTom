# Behavioral Evaluation System Technical Documentation

## Overview & Architecture

the behavioral evaluation system is a complete departure from mechanistic analysis—it's pure performance assessment without any activation caching or patching overhead. the codebase uses a factory pattern with industrial-grade logging to evaluate theory of mind reasoning across different models, temperatures, and prompt formats.

### Core Design Philosophy
- **Behavioral First**: No mechanistic analysis complexity—just generation + accuracy measurement
- **Industrial Logging**: Full wandb integration with structured artifacts and metadata
- **Modular Templates**: Unified template system replacing legacy scattered approaches
- **Configuration-Driven**: Predefined configs eliminate parameter soup anti-patterns

### File Structure & Relationships
```
behavioral/
├── behavioral_config.py        # Configuration classes & presets
├── behavioral_eval.py          # Main orchestration script  
├── behavioral_utils.py         # Evaluator, wandb logger, results saver
├── unified_prompt_builder.py   # Modern template-based prompt generation
├── unified_templates.py        # Template definitions & vocabulary system
├── test_basic.py              # Functionality verification
├── legacy/                    # Deprecated components (kept for reference)
└── results/                   # Local output directory
    └── tom_performance/
```

## Configuration System (behavioral_config.py)

### BehavioralConfig Dataclass
central configuration using dataclasses with smart defaults and __post_init__ validation:

```python
@dataclass
class BehavioralConfig:
    # Model settings
    models: List[str]                    # HF model names
    device_map: str = "cuda"             # Device mapping strategy  
    device: str = "cuda"                 # Target device (must match device_map for single-device)
    
    # Experimental parameters  
    temperatures: List[float] = None     # Default: [0.1, 0.4, 0.7, 1.0, 1.3, 1.6, 1.9]
    samples_per_condition: int = 10      # Responses per prompt
    vignette_types: List[str] = None     # ['false_belief', 'true_belief']
    tom_formats: List[str] = None        # ['direct', 'multiple_choice']
    
    # Question format settings
    question_format: str = "dual"        # "single" or "dual" 
    single_question_type: str = "mixed"  # "belief", "world", "mixed"
    
    # Template system
    template_names: List[str] = None     # Specific scenario templates
    prompt_variants: List[str] = None    # ['standard', 'detailed']
```

### Critical Device Configuration
the system previously had contradictory device settings causing cuda/cpu tensor mismatch errors:
```python
# FIXED: Consistent device configuration
device_map: str = "cuda"    # Was: "cpu" 
device: str = "cuda"        # Was: "cpu" (while expecting cuda tensors)
```

### DefaultBehavioralConfig Factory
predefined configurations eliminate parameter guessing:

- **quick_test()**: Fast validation (5 samples, limited temps)
- **full_comparison()**: Production comparison (Qwen vs Llama targets)  
- **temperature_sweep()**: Detailed temperature analysis
- **single_question_test()**: Single question format validation
- **wandb_test()**: Minimal wandb integration test

## Prompt Generation System

### Unified vs Legacy Approaches

**Modern (Unified)**: single template system with vocabulary substitution
- `unified_templates.py`: ScenarioTemplate dataclass with variable substitution
- `unified_prompt_builder.py`: Factory pattern for Prompt objects
- supports 6 template types: basic_object_move, food_truck, hair_styling, library_book, etc.

**Legacy (Deprecated)**: scattered builders in `legacy/` directory
- `behavioral_prompt_generator.py`: Original simple prompts
- `scenario_templates.py`: Hardcoded scenario lists
- kept for reference but not used in current system

### Template System Architecture

```python
@dataclass  
class ScenarioTemplate:
    name: str
    false_belief_template: str      # Template with {variable} placeholders
    true_belief_template: str       
    variables: Dict[str, str]       # variable_name -> vocab_file_name
    belief_answer_key: str          # Which variable contains belief answer
    world_answer_key: str           # Which variable contains world answer
```

### Vocabulary Loading
`VocabularyLoader` handles file-based vocabularies with fallbacks:
- loads from `tom_datasets/*.txt` files (locations.txt, foods.txt, etc.)
- graceful fallback to hardcoded lists if files missing
- `sample_different()` ensures location_1 ≠ location_2 for object movement scenarios

### Question Formatting
`UnifiedQuestionFormatter` provides template-specific questions:
- **standard**: terse questions ("agent looks where?")
- **detailed**: full sentences ("When Alice returns, where would she look?")
- **dual vs single**: different schema for belief/world vs single question formats

## Evaluation Pipeline & Data Flow

### Main Orchestration (behavioral_eval.py)

```python
def main():
    args = get_args()                           # CLI parsing
    config = create_config_from_args(args)      # Config instantiation
    set_seed(config.seed)                       # Reproducibility
    
    # Initialize utilities
    evaluator = BehavioralEvaluator(config)
    wandb_logger = WandbLogger(config) 
    results_saver = ResultsSaver(config)
    
    # Per-model evaluation loop
    for model_name in config.models:
        for template_name in config.template_names:
            for vignette_type in config.vignette_types:
                run_model_evaluation(...)       # Core evaluation
```

### Model Loading Integration
reuses `cma_config.py` and `models.py` from mechanistic analysis for consistent model loading:
```python
model_config = ModelConfig(model_type=model_name, device_map=config.device_map)
model_loader = ModelLoader(model_config, generation_config, prompt_config)
model, tokenizer, generation_kwargs, eos_token_ids, A_tok_id, B_tok_id, model_id = (
    model_loader.load_model_and_tokenizer()
)
```

### Generation & Evaluation Loop
per temperature condition:
1. **Prompt Generation**: `batch_generate()` creates Prompt objects with metadata
2. **Token Placement**: `.to(model.cfg.device)` ensures tensor device consistency  
3. **Generation**: `model.generate()` with temperature/sampling parameters
4. **Response Parsing**: `extract_dual_answers()` regex extraction of belief/world
5. **Accuracy Calculation**: normalized location comparison via `normalize_loc()`

### Prompt Object Structure
```python
@dataclass
class Prompt:
    text: str                    # Full prompt text with schema
    expected_belief: str         # Expected belief answer (location)
    expected_world: str          # Expected world answer (location)  
    metadata: Dict[str, str]     # vignette_type, format_type, template_name, etc.
```

## Wandb Integration & Logging

### WandbLogger Architecture
sophisticated logging with authentication fallbacks and structured artifacts:

```python
class WandbLogger:
    def _setup_wandb(self):
        # Try WANDB_API_KEY env var
        # Fallback to codebase/utils.py WANDB_API_KEY
        # Graceful degradation if auth fails
    
    def init_experiment(self, model_name, vignette_type, tom_format, ...):
        # Per-run initialization with rich metadata
        # Smart run naming: "llama-7b_food-truck_fb_dual"
        
    def log_condition_results(self, results, temperature, prompt_response_pairs):
        # Temperature-step logging for meaningful x-axis
        # Structured artifact creation with JSON responses
```

### Metadata Tagging Strategy
extensive tagging for filtering in wandb interface:
- **Model**: family (llama/qwen), size (7b/14b), is_instruct
- **Experimental**: vignette_type, tom_format, template_name, context_type
- **Format**: question_format_dual/single, single_type_mixed/belief/world
- **Task**: behavioral_eval, theory_of_mind

### Artifact Structure
JSON artifacts with cleaned response data:
```json
{
  "run_metadata": {
    "temperature": 0.7,
    "vignette_type": "false_belief",
    "model_name": "meta-llama/Llama-3.1-7B-Instruct"
  },
  "responses": [
    {
      "prompt": "object at kitchen. agent leaves...",
      "expected_belief": "kitchen", 
      "expected_world": "garden",
      "cleaned_response": {
        "belief": "kitchen",
        "world": "garden", 
        "belief_correct": true,
        "world_correct": true
      }
    }
  ]
}
```

## Template System & Scenario Generation

### ScenarioTemplate Examples

**basic_object_move**:
```python
false_belief_template="object at {location_1}. agent leaves room. object moved to {location_2}."
variables={"location_1": "locations", "location_2": "locations"}
belief_answer_key="location_1"  # Agent thinks object still at original location
world_answer_key="location_2"   # Object actually at new location
```

**food_truck** (naturalistic):
```python
false_belief_template="Alice driving past food truck on {street_1}. Alice continues down road. After Alice out of sight, Bob drives truck to {street_2}..."
variables={"food": "foods", "street_1": "streets", "street_2": "streets"}
```

### Variable Substitution Logic
smart handling of paired variables to ensure distinctness:
```python
# Ensures location_1 ≠ location_2 
if var_name.endswith("_1") and var_name.replace("_1", "_2") in template.variables:
    val_1, val_2 = self.vocab_loader.sample_different(vocab_name, 2)
```

### True vs False Belief Generation
- **False Belief**: agent misses object movement → belief_answer ≠ world_answer
- **True Belief**: agent witnesses movement → belief_answer = world_answer (both correct)

## Key Classes & Responsibilities

### BehavioralEvaluator
response evaluation without mechanistic complexity:
```python
def evaluate_response(self, response: str, prompt) -> Dict[str, Any]:
    belief_answer, world_answer = extract_dual_answers(response)
    belief_correct = normalize_loc(belief_answer) == normalize_loc(prompt.expected_belief)
    world_correct = normalize_loc(world_answer) == normalize_loc(prompt.expected_world)
    
    return {
        'correct': belief_correct and world_correct,
        'belief_correct': belief_correct, 
        'world_correct': world_correct,
        'malformed': malformed
    }
```

### ResultsSaver
local JSON persistence with timestamp-based filenames:
- **Experiment Results**: complete results per model with serialization handling
- **Raw Responses**: detailed response logs per condition for analysis
- **JSON Compatibility**: numpy array and tensor conversion for serialization

### UnifiedPromptBuilder
factory pattern for prompt generation:
```python
def batch_generate(self, num_prompts, template_names, vignette_types, ...):
    # Special handling for paired belief/world questions
    if "belief" in question_types and "world" in question_types:
        return self._generate_paired_questions(...)
    
    # Regular combinatorial generation
    combinations = [(template, vignette, question, format_type, variant) ...]
```

## Command Line Interface & Usage

### Primary CLI Arguments
```bash
# Experiment type selection
--config_type {quick_test,full_comparison,temperature_sweep,single_question_test}
--single_model "model_name"  # For temperature_sweep

# Model & experimental parameters  
--models "model1" "model2"
--temperatures 0.1 0.4 0.7
--samples_per_condition 10
--prompt_num 50

# Task configuration
--vignette_types false_belief true_belief
--tom_formats direct multiple_choice
--question_format {single,dual}
--single_question_type {belief,world,mixed}

# Template selection
--template_names food_truck hair_styling basic_object_move
--scenario_type {basic,naturalistic,mixed}

# Logging control
--wandb_project "project_name"
--no_wandb                   # Disable wandb logging
--save_dir "results/custom"
```

### Common Usage Patterns
```bash
# Quick validation
python behavioral_eval.py --config_type quick_test --no_wandb

# Production comparison  
python behavioral_eval.py --config_type full_comparison

# Temperature analysis
python behavioral_eval.py --config_type temperature_sweep --single_model "Qwen2.5-14B-Instruct"

# Single question format testing
python behavioral_eval.py --config_type single_question_test --question_format single --single_question_type mixed
```

## Data Structures & Formats

### Core Data Types
```python
# Configuration
@dataclass BehavioralConfig      # Main config with validation
@dataclass ScenarioTemplate     # Template definition
@dataclass Prompt               # Single prompt + metadata

# Results  
Dict[str, Any] condition_results # Per-temperature results
List[Dict] raw_responses        # Detailed response logs
Dict[str, Dict] all_model_results # Complete experiment results
```

### Response Evaluation Schema
```python
{
    'belief_accuracy': float,      # Belief prediction accuracy [0,1]
    'world_accuracy': float,       # World state accuracy [0,1] 
    'malformed_rate': float,       # Parsing failure rate [0,1]
    'total_responses': int,        # Number of generated responses
    'belief_correct_count': int,   # Raw correct counts
    'world_correct_count': int,
    'raw_responses': [             # Detailed per-response data
        {
            'prompt': str,
            'response': str,
            'expected_belief': str,
            'expected_world': str,
            'belief_correct': bool,
            'world_correct': bool,
            'malformed': bool,
            'metadata': dict
        }
    ]
}
```

### File Output Structure
```
results/tom_performance/
├── model_name_timestamp_behavioral_results.json    # Per-model results
├── combined_results_timestamp_behavioral_results.json  # All models
└── raw_responses/
    └── model_T0.7_false_belief_direct_responses_timestamp.json
```

## Integration with Larger Codebase

### Mechanistic Analysis Separation
completely independent from CMA pipeline:
- **No Shared State**: separate model loading, prompt generation, evaluation
- **Common Utilities**: reuses `models.py`, `cma_config.py` for consistency  
- **Identical Prompts**: same template system enables comparative analysis
- **Performance First**: fast iteration without GPU-intensive caching

### Model Loading Compatibility
leverages existing model loading infrastructure:
```python
# Reuses CMA model loading with minimal config
from codebase.tasks.identity_rules.cma_config import ModelConfig, ExperimentConfig
from codebase.tasks.identity_rules.models import ModelLoader

# Creates minimal experiment config just for model loading
exp_config = ExperimentConfig(
    model=model_config,
    generation=GenerationConfig(max_new_tokens=50),
    prompts=PromptConfig(base_rule="ABA", context_type="abstract"),
    evaluation=EvaluationConfig(eval_metric="gen_acc"),
    patching=None  # No patching needed
)
```

### Template System Evolution
progression from scattered to unified:
1. **Legacy**: hardcoded scenarios in separate files
2. **Current**: unified template system with vocabulary substitution
3. **Future**: easy extension with new template types

## Troubleshooting & Common Issues

### Device Mismatch Errors
**Problem**: "Expected all tensors to be on the same device, but found at least two devices"
**Solution**: ensure device_map and device settings match in BehavioralConfig

### Wandb Authentication 
**Problem**: wandb login failures in HPC environments
**Solution**: set WANDB_API_KEY environment variable or use --no_wandb flag

### Template Not Found
**Problem**: unknown template name errors
**Solution**: use `list_available_templates()` or check unified_templates.py definitions

### Memory Issues
**Problem**: OOM errors with large models
**Solution**: reduce batch_size and samples_per_condition in config

### Import Errors
**Problem**: module not found errors
**Solution**: verify path setup in behavioral_eval.py and run from tom_dev root directory

## Performance Characteristics

### Evaluation Speed
- **No Caching Overhead**: pure generation without activation storage
- **Batch Processing**: configurable batch sizes for memory management
- **Temperature Parallelization**: sequential but efficient per-temperature evaluation

### Memory Usage  
- **Model Loading**: standard transformer memory requirements
- **Response Storage**: minimal memory for response strings  
- **No Activation Cache**: dramatically reduced memory vs mechanistic analysis

### Scalability
- **Model Count**: linear scaling with number of models
- **Temperature Range**: linear scaling with temperature conditions
- **Sample Size**: configurable for speed vs statistical power tradeoff

this system provides production-ready behavioral evaluation with industrial logging and zero mechanistic analysis overhead—perfect for rapid model comparison and performance characterization.

## Recent Fixes & Improvements

### Parsing Fix (January 2025)
Fixed dual-answer extraction that was failing due to regex alternation issues:
- **Problem**: `extract_dual_answers()` couldn't parse "where the agent thinks: <loc>X</loc>" format
- **Solution**: Replaced complex alternation regex with separate BELIEF_PATTERNS and WORLD_PATTERNS lists
- **Result**: Now correctly extracts belief="X", world="Y" from model responses

### Template Separation Fix (January 2025) 
Fixed wandb run organization that was mixing all templates together:
- **Problem**: Single wandb run per model labeled as "mixed" containing all templates
- **Solution**: Separate wandb runs per template+vignette combination
- **Result**: Clean artifacts with runs like "llama-7b_food-truck_fb" for easy comparison

### Single Question Format
For mixed single questions, system generates identical scenarios twice with different question types:
- Same locations (loc1, loc2) used for both belief and world questions
- Paired generation ensures true comparison of question format effects
- No shuffling between belief/world pairs to maintain pairing

This documentation serves as a complete reference for understanding, using, and modifying the behavioral evaluation system.